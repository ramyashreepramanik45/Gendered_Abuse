{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11146901,"sourceType":"datasetVersion","datasetId":6953916,"isSourceIdPinned":false},{"sourceId":11148331,"sourceType":"datasetVersion","datasetId":6955052},{"sourceId":11148812,"sourceType":"datasetVersion","datasetId":6955426}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:13:20.975548Z","iopub.execute_input":"2025-03-25T12:13:20.976011Z","iopub.status.idle":"2025-03-25T12:13:20.980176Z","shell.execute_reply.started":"2025-03-25T12:13:20.975971Z","shell.execute_reply":"2025-03-25T12:13:20.979275Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Preprocess english dataset","metadata":{}},{"cell_type":"code","source":"eng_df = pd.read_csv('/kaggle/input/hate-speech-english/english_dataset.tsv', sep='\\t')\neng_df = eng_df.drop(columns=['text_id','task_2', 'task_3'])\n\neng_df = eng_df.rename(columns={'task_1': 'label'})\n\n# Map label values\nlabel_map = {'NOT': 0, 'HOF': 1}\neng_df['label'] = eng_df['label'].map(label_map)\neng_df.head\neng_train, eng_val = train_test_split(eng_df, test_size=0.3, random_state=42, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:13:20.982954Z","iopub.execute_input":"2025-03-25T12:13:20.983182Z","iopub.status.idle":"2025-03-25T12:13:21.034074Z","shell.execute_reply.started":"2025-03-25T12:13:20.983157Z","shell.execute_reply":"2025-03-25T12:13:21.033186Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import re,string\ndef normalize_text(text):\n  # text = text.lower()\n  text = re.sub('\\[.*?\\]', ' ', text)\n  text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n  text = re.sub('<.*?>+', ' ', text)\n  text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n  text = re.sub('\\n', ' ', text)\n  text = re.sub('\\w*\\d\\w*', ' ', text)\n  text = re.sub('<handle replaced>', '', text)\n  return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:13:21.035200Z","iopub.execute_input":"2025-03-25T12:13:21.035539Z","iopub.status.idle":"2025-03-25T12:13:21.040033Z","shell.execute_reply.started":"2025-03-25T12:13:21.035509Z","shell.execute_reply":"2025-03-25T12:13:21.039168Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"eng_train['text'] = eng_train['text'].apply(lambda x: normalize_text(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:13:21.041904Z","iopub.execute_input":"2025-03-25T12:13:21.042144Z","iopub.status.idle":"2025-03-25T12:13:21.197660Z","shell.execute_reply.started":"2025-03-25T12:13:21.042124Z","shell.execute_reply":"2025-03-25T12:13:21.196747Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"eng_val['text'] = eng_val['text'].apply(lambda x: normalize_text(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:13:21.198903Z","iopub.execute_input":"2025-03-25T12:13:21.199155Z","iopub.status.idle":"2025-03-25T12:13:21.266065Z","shell.execute_reply.started":"2025-03-25T12:13:21.199133Z","shell.execute_reply":"2025-03-25T12:13:21.265266Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = \"ai4bharat/indic-bert\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nclass TextDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len=128):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        text = str(self.data.iloc[idx]['text'])\n        label = int(self.data.iloc[idx]['label'])\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(label)\n        }\n\n\ntrain_dataset = TextDataset(eng_train, tokenizer)\nval_dataset = TextDataset(eng_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:13:21.266877Z","iopub.execute_input":"2025-03-25T12:13:21.267122Z","iopub.status.idle":"2025-03-25T12:13:34.510418Z","shell.execute_reply.started":"2025-03-25T12:13:21.267101Z","shell.execute_reply":"2025-03-25T12:13:34.509735Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c02b3c84f54157b5b911bb5346c40e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9c89e406de64de18e0f92401875fa8e"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = nn.CrossEntropyLoss()\n\ndef train_epoch(model, loader):\n    model.train()\n    total_loss = 0\n    loop = tqdm(loader, desc=\"Training\")\n    for batch in loop:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())  # Show loss in tqdm bar\n    return total_loss / len(loader)\n\ndef eval_model(model, loader):\n    model.eval()\n    preds, labels_list = [], []\n    loop = tqdm(loader, desc=\"Evaluating\")\n    with torch.no_grad():\n        for batch in loop:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n            labels_list.extend(labels.cpu().numpy())\n    f1 = f1_score(labels_list, preds, average='macro')\n    return f1\n\nfor epoch in range(3):\n    print(f\"Epoch {epoch+1}\")\n    train_loss = train_epoch(model, train_loader)\n    val_f1 = eval_model(model, val_loader)\n    print(f\"Train Loss = {train_loss:.4f}, Val F1 = {val_f1:.4f}\")\n\ntorch.save(model.state_dict(), \"stage1_hate_speech.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:13:34.511175Z","iopub.execute_input":"2025-03-25T12:13:34.511677Z","iopub.status.idle":"2025-03-25T12:16:33.951129Z","shell.execute_reply.started":"2025-03-25T12:13:34.511640Z","shell.execute_reply":"2025-03-25T12:16:33.950320Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450efe276dbf41ff98f271d07c29cf8e"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 256/256 [00:48<00:00,  5.24it/s, loss=0.673]\nEvaluating: 100%|██████████| 55/55 [00:06<00:00,  8.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss = 0.6673, Val F1 = 0.3810\nEpoch 2\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 256/256 [00:48<00:00,  5.33it/s, loss=0.7]  \nEvaluating: 100%|██████████| 55/55 [00:06<00:00,  8.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss = 0.6427, Val F1 = 0.5902\nEpoch 3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 256/256 [00:47<00:00,  5.34it/s, loss=0.657]\nEvaluating: 100%|██████████| 55/55 [00:06<00:00,  8.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss = 0.6165, Val F1 = 0.6268\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def data_preprocessing(df, language):\n    # Select annotator columns based on language\n    if language == \"tamil\":\n        annotator_cols = [\"ta_a1\", \"ta_a2\", \"ta_a3\", \"ta_a4\", \"ta_a5\", \"ta_a6\"]\n    elif language == \"hindi\":\n        annotator_cols = [\"hi_a1\", \"hi_a2\", \"hi_a3\", \"hi_a4\", \"hi_a5\"]\n    else:\n        annotator_cols = [\"en_a1\", \"en_a2\", \"en_a3\", \"en_a4\", \"en_a5\", \"en_a6\"]\n\n    df[annotator_cols] = df[annotator_cols].fillna(\"\")\n    def majority_label(row):\n        votes = []\n        for col in annotator_cols:\n            val = row[col]\n            try:\n                if val != \"\":\n                    num_val = float(val)\n                    if num_val in [0.0, 1.0]:\n                        votes.append(int(num_val))\n            except:\n                continue  \n        if not votes:\n            return 0\n        return 1 if votes.count(1) > votes.count(0) else 0\n\n    # Apply the majority label function\n    df[\"label\"] = df.apply(majority_label, axis=1)\n\n    # Ensure label column is integer type\n    df[\"label\"] = df[\"label\"].astype(int)\n\n    # Drop annotation columns and unnecessary 'key' column\n    df = df.drop(columns=annotator_cols + [\"key\"])\n\n    # Show label distribution\n    print(df[\"label\"].value_counts())\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:16:33.952295Z","iopub.execute_input":"2025-03-25T12:16:33.953444Z","iopub.status.idle":"2025-03-25T12:16:33.959890Z","shell.execute_reply.started":"2025-03-25T12:16:33.953415Z","shell.execute_reply":"2025-03-25T12:16:33.959051Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df_eng = pd.read_csv(\"/kaggle/input/gender-abuse-dataset/train_en_l1.csv\")\nrows, columns = df_eng.shape\nprint(f\"Rows: {rows}, Columns: {columns}\")\neng_train_gender = data_preprocessing(df_eng,\"english\")\neng_train_gender['text'] = eng_train_gender['text'].apply(lambda x: normalize_text(x))\nrows, columns = eng_train_gender.shape\nprint(f\"Rows: {rows}, Columns: {columns}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:16:33.962174Z","iopub.execute_input":"2025-03-25T12:16:33.962392Z","iopub.status.idle":"2025-03-25T12:16:34.288331Z","shell.execute_reply.started":"2025-03-25T12:16:33.962372Z","shell.execute_reply":"2025-03-25T12:16:34.287553Z"}},"outputs":[{"name":"stdout","text":"Rows: 6531, Columns: 8\nlabel\n0    5269\n1    1262\nName: count, dtype: int64\nRows: 6531, Columns: 2\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_stage2(model, train_loader, epochs=3):\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    for epoch in range(epochs):\n        total_loss = 0\n        loop = tqdm(train_loader, desc=f\"Training Stage 2 - Epoch {epoch+1}\")\n        for batch in loop:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            loop.set_postfix(loss=loss.item())\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}: Train Loss = {avg_loss:.4f}\")\n\n    # Save fine-tuned model\n    torch.save(model.state_dict(), \"stage2_gendered_abuse.pth\")\n    print(\"Stage 2 Model Saved: stage2_gendered_abuse.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:16:34.289317Z","iopub.execute_input":"2025-03-25T12:16:34.289580Z","iopub.status.idle":"2025-03-25T12:16:34.295375Z","shell.execute_reply.started":"2025-03-25T12:16:34.289545Z","shell.execute_reply":"2025-03-25T12:16:34.294569Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"gendered_train_dataset = TextDataset(eng_train_gender, tokenizer)\ngendered_train_loader = DataLoader(gendered_train_dataset, batch_size=16, shuffle=True)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/stage1_hate_speech.pth\"))\n\n\ntrain_loss = train_stage2(model, gendered_train_loader)\n\ntorch.save(model.state_dict(), \"stage2_gendered_abuse.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:16:34.296209Z","iopub.execute_input":"2025-03-25T12:16:34.296498Z","iopub.status.idle":"2025-03-25T12:20:24.381956Z","shell.execute_reply.started":"2025-03-25T12:16:34.296467Z","shell.execute_reply":"2025-03-25T12:20:24.380883Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-13-0c2caa812cfb>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/stage1_hate_speech.pth\"))\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nTraining Stage 2 - Epoch 1: 100%|██████████| 409/409 [01:16<00:00,  5.35it/s, loss=0.186]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 0.4190\n","output_type":"stream"},{"name":"stderr","text":"Training Stage 2 - Epoch 2: 100%|██████████| 409/409 [01:16<00:00,  5.35it/s, loss=0.232] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 0.3477\n","output_type":"stream"},{"name":"stderr","text":"Training Stage 2 - Epoch 3: 100%|██████████| 409/409 [01:16<00:00,  5.35it/s, loss=0.539] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss = 0.3027\nStage 2 Model Saved: stage2_gendered_abuse.pth\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"df_eng_test = pd.read_csv(\"/kaggle/input/gender-abuse-dataset/test_en_l1.csv\")\nrows, columns = df_eng_test.shape\nprint(f\"Rows: {rows}, Columns: {columns}\")\neng_test_gender = data_preprocessing(df_eng_test,\"english\")\nrows, columns = eng_test_gender.shape\nprint(f\"Rows: {rows}, Columns: {columns}\")\n\ngendered_test_dataset = TextDataset(df_eng_test, tokenizer)\ngendered_test_loader = DataLoader(gendered_test_dataset, batch_size=16, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:20:24.382821Z","iopub.execute_input":"2025-03-25T12:20:24.383130Z","iopub.status.idle":"2025-03-25T12:20:24.424950Z","shell.execute_reply.started":"2025-03-25T12:20:24.383100Z","shell.execute_reply":"2025-03-25T12:20:24.424276Z"}},"outputs":[{"name":"stdout","text":"Rows: 1107, Columns: 8\nlabel\n0    877\n1    230\nName: count, dtype: int64\nRows: 1107, Columns: 2\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n\nmodel.load_state_dict(torch.load(\"stage2_gendered_abuse.pth\"))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:20:24.425660Z","iopub.execute_input":"2025-03-25T12:20:24.425935Z","iopub.status.idle":"2025-03-25T12:20:25.261422Z","shell.execute_reply.started":"2025-03-25T12:20:24.425914Z","shell.execute_reply":"2025-03-25T12:20:25.260243Z"}},"outputs":[{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-15-0f5f36068eb1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"stage2_gendered_abuse.pth\"))\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"AlbertForSequenceClassification(\n  (albert): AlbertModel(\n    (embeddings): AlbertEmbeddings(\n      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (encoder): AlbertTransformer(\n      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n      (albert_layer_groups): ModuleList(\n        (0): AlbertLayerGroup(\n          (albert_layers): ModuleList(\n            (0): AlbertLayer(\n              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (attention): AlbertSdpaAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (attention_dropout): Dropout(p=0, inplace=False)\n                (output_dropout): Dropout(p=0, inplace=False)\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              )\n              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n              (activation): GELUActivation()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (pooler): Linear(in_features=768, out_features=768, bias=True)\n    (pooler_activation): Tanh()\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report\nfrom tqdm import tqdm\n\ndef inference_on_gendered_abuse_test(model, test_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing on Gendered Abuse\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # F1 Score (Macro)\n    f1_macro = f1_score(all_labels, all_preds, average='macro')\n    print(f\"\\nTest F1 Score (Macro): {f1_macro:.4f}\")\n    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, digits=4))\n    return f1_macro\n\nf1_macro_test = inference_on_gendered_abuse_test(model, gendered_test_loader)\nprint(f\"Final Gendered Abuse Test F1 Score (Macro): {f1_macro_test:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:20:25.262788Z","iopub.execute_input":"2025-03-25T12:20:25.263127Z","iopub.status.idle":"2025-03-25T12:20:29.557050Z","shell.execute_reply.started":"2025-03-25T12:20:25.263088Z","shell.execute_reply":"2025-03-25T12:20:29.556169Z"}},"outputs":[{"name":"stderr","text":"                                                                          ","output_type":"stream"},{"name":"stdout","text":"\nTest F1 Score (Macro): 0.6097\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0     0.8253    0.9749    0.8939       877\n           1     0.6901    0.2130    0.3256       230\n\n    accuracy                         0.8166      1107\n   macro avg     0.7577    0.5940    0.6097      1107\nweighted avg     0.7972    0.8166    0.7758      1107\n\nFinal Gendered Abuse Test F1 Score (Macro): 0.6097\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":16}]}