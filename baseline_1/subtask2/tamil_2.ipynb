{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11146901,"sourceType":"datasetVersion","datasetId":6953916},{"sourceId":11152256,"sourceType":"datasetVersion","datasetId":6957969},{"sourceId":11152320,"sourceType":"datasetVersion","datasetId":6958017},{"sourceId":11148331,"sourceType":"datasetVersion","datasetId":6955052}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ntamil_train = pd.read_csv(\"/kaggle/input/hate-speech/tamil_train.csv\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-03-25T12:32:52.678668Z","iopub.execute_input":"2025-03-25T12:32:52.678966Z","iopub.status.idle":"2025-03-25T12:32:52.761546Z","shell.execute_reply.started":"2025-03-25T12:32:52.678943Z","shell.execute_reply":"2025-03-25T12:32:52.760916Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import re,string\ndef normalize_text(text):\n  # text = text.lower()\n  text = re.sub('\\[.*?\\]', ' ', text)\n  text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n  text = re.sub('<.*?>+', ' ', text)\n  text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n  text = re.sub('\\n', ' ', text)\n  text = re.sub('\\w*\\d\\w*', ' ', text)\n  text = re.sub('<handle replaced>', '', text)\n  return text","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:32:52.762700Z","iopub.execute_input":"2025-03-25T12:32:52.762955Z","iopub.status.idle":"2025-03-25T12:32:52.767512Z","shell.execute_reply.started":"2025-03-25T12:32:52.762923Z","shell.execute_reply":"2025-03-25T12:32:52.766753Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tamil_train['text'] = tamil_train['text'].apply(lambda x: normalize_text(x))\ntamil_train.head","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:32:52.769144Z","iopub.execute_input":"2025-03-25T12:32:52.769359Z","iopub.status.idle":"2025-03-25T12:32:53.043381Z","shell.execute_reply.started":"2025-03-25T12:32:52.769341Z","shell.execute_reply":"2025-03-25T12:32:53.042598Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of        label                                               text\n0          1                          அம்மு காலை வணக்கம் அழகுடி\n1          1          அசத்தலான அழகு நடிப்பு சூப்பர் ப்ரண்ட்ஸ் 💖\n2          0             இப் புண்டா மவனே வீடியோ அளி டி தேவிடியா\n3          1                      சானி அள்ளு போ காஞ்சிறப்போகுது\n4          1                          மிகவும் அழகான இளவரசி 💖👸💖✨\n...      ...                                                ...\n17995      0  அடியே மூதேவி உனக்கு கொஞ்சம் கூட மானம் மரியாதை ...\n17996      0                பெருசா இருக்கு  ரெண்டு காய் d ponda\n17997      0  உப்பு   செருப்பு   சிரிப்பு ஏதும் இல்லை    பூ ...\n17998      1                           நீ நயண்தாரா தங்கச்சியா 😜\n17999      1  நீங்கள் புடவை கட்டும் விதம்  சூப்பர் என்று சொல...\n\n[18000 rows x 2 columns]>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"tamil_val = pd.read_csv(\"/kaggle/input/hate-speech/tamil_val.csv\")\ntamil_val['text'] = tamil_val['text'].apply(lambda x: normalize_text(x))","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:32:53.044699Z","iopub.execute_input":"2025-03-25T12:32:53.044953Z","iopub.status.idle":"2025-03-25T12:32:53.190536Z","shell.execute_reply.started":"2025-03-25T12:32:53.044931Z","shell.execute_reply":"2025-03-25T12:32:53.189596Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = \"ai4bharat/indic-bert\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nclass TextDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len=128):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        text = str(self.data.iloc[idx]['text'])\n        label = int(self.data.iloc[idx]['label'])\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(label)\n        }\n\n\ntrain_dataset = TextDataset(tamil_train, tokenizer)\nval_dataset = TextDataset(tamil_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:32:53.191481Z","iopub.execute_input":"2025-03-25T12:32:53.191805Z","iopub.status.idle":"2025-03-25T12:33:03.894299Z","shell.execute_reply.started":"2025-03-25T12:32:53.191770Z","shell.execute_reply":"2025-03-25T12:33:03.893542Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3df031bdb37440ebe29962ad1d4b128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84812ab2b1243fb883f298091a52ac1"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = nn.CrossEntropyLoss()\n\ndef train_epoch(model, loader):\n    model.train()\n    total_loss = 0\n    loop = tqdm(loader, desc=\"Training\")\n    for batch in loop:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    return total_loss / len(loader)\n\ndef eval_model(model, loader):\n    model.eval()\n    preds, labels_list = [], []\n    loop = tqdm(loader, desc=\"Evaluating\")\n    with torch.no_grad():\n        for batch in loop:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n            labels_list.extend(labels.cpu().numpy())\n    f1 = f1_score(labels_list, preds, average='macro')\n    return f1\n\nfor epoch in range(3):\n    print(f\"Epoch {epoch+1}\")\n    train_loss = train_epoch(model, train_loader)\n    val_f1 = eval_model(model, val_loader)\n    print(f\"Train Loss = {train_loss:.4f}, Val F1 = {val_f1:.4f}\")\n\ntorch.save(model.state_dict(), \"stage1_hate_speech.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:33:03.895228Z","iopub.execute_input":"2025-03-25T12:33:03.895783Z","iopub.status.idle":"2025-03-25T12:44:58.585196Z","shell.execute_reply.started":"2025-03-25T12:33:03.895750Z","shell.execute_reply":"2025-03-25T12:44:58.584162Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f7059efaa54f92ac0771562da1ce96"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1125/1125 [03:31<00:00,  5.31it/s, loss=0.371]\nEvaluating: 100%|██████████| 188/188 [00:22<00:00,  8.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss = 0.5236, Val F1 = 0.8118\nEpoch 2\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1125/1125 [03:30<00:00,  5.33it/s, loss=0.215] \nEvaluating: 100%|██████████| 188/188 [00:22<00:00,  8.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss = 0.3570, Val F1 = 0.8299\nEpoch 3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1125/1125 [03:30<00:00,  5.33it/s, loss=0.0775]\nEvaluating: 100%|██████████| 188/188 [00:22<00:00,  8.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss = 0.3046, Val F1 = 0.8344\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def data_preprocessing(df, language):\n    if language == \"tamil\":\n        annotator_cols = [\"ta_a1\", \"ta_a2\", \"ta_a3\", \"ta_a4\", \"ta_a5\", \"ta_a6\"]\n    elif language == \"hindi\":\n        annotator_cols = [\"hi_a1\", \"hi_a2\", \"hi_a3\", \"hi_a4\", \"hi_a5\"]\n    else:\n        annotator_cols = [\"en_a1\", \"en_a2\", \"en_a3\", \"en_a4\", \"en_a5\", \"en_a6\"]\n\n    df[annotator_cols] = df[annotator_cols].fillna(\"\")\n    def majority_label(row):\n        votes = []\n        for col in annotator_cols:\n            val = row[col]\n            try:\n                if val != \"\":\n                    num_val = float(val)\n                    if num_val in [0.0, 1.0]:\n                        votes.append(int(num_val))\n            except:\n                continue  \n        if not votes:\n            return 0\n        return 1 if votes.count(1) > votes.count(0) else 0\n\n    df[\"label\"] = df.apply(majority_label, axis=1)\n    df[\"label\"] = df[\"label\"].astype(int)\n    df = df.drop(columns=annotator_cols + [\"key\"])\n\n    # Show label distribution\n    print(df[\"label\"].value_counts())\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:44:58.586387Z","iopub.execute_input":"2025-03-25T12:44:58.587544Z","iopub.status.idle":"2025-03-25T12:44:58.594415Z","shell.execute_reply.started":"2025-03-25T12:44:58.587504Z","shell.execute_reply":"2025-03-25T12:44:58.593694Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df_tamil = pd.read_csv(\"/kaggle/input/gender-abuse-dataset/train_ta_l1.csv\")\nrows, columns = df_tamil.shape\nprint(f\"Rows: {rows}, Columns: {columns}\")\ntamil_train_gender = data_preprocessing(df_tamil,\"tamil\")\ntamil_train_gender['text'] = tamil_train_gender['text'].apply(lambda x: normalize_text(x))\nrows, columns = tamil_train_gender.shape\nprint(f\"Rows: {rows}, Columns: {columns}\")\ntamil_train_gender.head","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:44:58.595161Z","iopub.execute_input":"2025-03-25T12:44:58.595398Z","iopub.status.idle":"2025-03-25T12:44:58.990044Z","shell.execute_reply.started":"2025-03-25T12:44:58.595377Z","shell.execute_reply":"2025-03-25T12:44:58.989295Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Rows: 6779, Columns: 8\nlabel\n0    3890\n1    2889\nName: count, dtype: int64\nRows: 6779, Columns: 2\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of                                                    text  label\n0            முரசொலி அலுவலகம் அமைந்துள்ள இடம் பஞ்சமி...      0\n1        சோத்துக்கு பிச்சை எடுக்கிற கடங்கார நாய்களுக...      0\n2              தத்தபுத்த தத்தபுத்த ன்னு எதாவது புரியுதா      0\n3         பச்சை மொளகா காரம் vicky அம்மா புண்டை நாறும் 😆      1\n4       என்ன உடம்பு டா சாமி  சும்மா வளுவளுனு   முலை ...      1\n...                                                 ...    ...\n6774                      😭😭😭 ஒம்மாள படிக்கல் புண்ட 😭😭😭      1\n6775  🙄🙄🙄🙄 என்ன எழவுயா இது      இதெல்லாம் ஒரு பெருமை...      0\n6776  🚨எக்ஸ் பிரஸ் பேர்ல் கப்பல் தீ விபத்துக்கு உள்ள...      0\n6777  🤣 🤣 சல்லி ஜாதி வெறி முட்டா புண்ட உங்க பொண்ணுங்...      0\n6778  🤣🤣🤣 நீ சொல்றது எல்லாமும் அந்த திம்கவோட தம்பி  ...      1\n\n[6779 rows x 2 columns]>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_stage2(model, train_loader, epochs=3):\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    for epoch in range(epochs):\n        total_loss = 0\n        loop = tqdm(train_loader, desc=f\"Training Stage 2 - Epoch {epoch+1}\")\n        for batch in loop:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            loop.set_postfix(loss=loss.item())\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}: Train Loss = {avg_loss:.4f}\")\n\n    # Save fine-tuned model\n    torch.save(model.state_dict(), \"stage2_gendered_abuse.pth\")\n    print(\"Stage 2 Model Saved: stage2_gendered_abuse.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:44:58.992539Z","iopub.execute_input":"2025-03-25T12:44:58.992936Z","iopub.status.idle":"2025-03-25T12:44:58.998635Z","shell.execute_reply.started":"2025-03-25T12:44:58.992911Z","shell.execute_reply":"2025-03-25T12:44:58.997855Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"gendered_train_dataset = TextDataset(tamil_train_gender, tokenizer)\ngendered_train_loader = DataLoader(gendered_train_dataset, batch_size=16, shuffle=True)\n\nmodel.load_state_dict(torch.load(\"/kaggle/working/stage1_hate_speech.pth\"))\n\ntrain_loss = train_stage2(model, gendered_train_loader)\n\ntorch.save(model.state_dict(), \"stage2_gendered_abuse.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:44:58.999821Z","iopub.execute_input":"2025-03-25T12:44:59.000181Z","iopub.status.idle":"2025-03-25T12:48:59.758145Z","shell.execute_reply.started":"2025-03-25T12:44:59.000153Z","shell.execute_reply":"2025-03-25T12:48:59.757024Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-19-e970a2f70e41>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/stage1_hate_speech.pth\"))\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nTraining Stage 2 - Epoch 1: 100%|██████████| 424/424 [01:19<00:00,  5.30it/s, loss=0.571]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 0.5704\n","output_type":"stream"},{"name":"stderr","text":"Training Stage 2 - Epoch 2: 100%|██████████| 424/424 [01:20<00:00,  5.30it/s, loss=0.544]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 0.4623\n","output_type":"stream"},{"name":"stderr","text":"Training Stage 2 - Epoch 3: 100%|██████████| 424/424 [01:20<00:00,  5.30it/s, loss=0.43]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss = 0.4083\nStage 2 Model Saved: stage2_gendered_abuse.pth\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"df_tamil_test = pd.read_csv(\"/kaggle/input/gender-abuse-dataset/test_ta_l1.csv\")\nrows, columns = df_tamil_test.shape\nprint(f\"Rows: {rows}, Columns: {columns}\")\ntamil_test_gender = data_preprocessing(df_tamil_test,\"tamil\")\nrows, columns = tamil_test_gender.shape\nprint(f\"Rows: {rows}, Columns: {columns}\")\n\ngendered_test_dataset = TextDataset(tamil_test_gender, tokenizer)\ngendered_test_loader = DataLoader(gendered_test_dataset, batch_size=16, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:48:59.759290Z","iopub.execute_input":"2025-03-25T12:48:59.759617Z","iopub.status.idle":"2025-03-25T12:48:59.824467Z","shell.execute_reply.started":"2025-03-25T12:48:59.759587Z","shell.execute_reply":"2025-03-25T12:48:59.823718Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Rows: 1135, Columns: 8\nlabel\n0    596\n1    539\nName: count, dtype: int64\nRows: 1135, Columns: 2\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n\nmodel.load_state_dict(torch.load(\"stage2_gendered_abuse.pth\"))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:48:59.825298Z","iopub.execute_input":"2025-03-25T12:48:59.825522Z","iopub.status.idle":"2025-03-25T12:49:00.438820Z","shell.execute_reply.started":"2025-03-25T12:48:59.825503Z","shell.execute_reply":"2025-03-25T12:49:00.437767Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-21-0f5f36068eb1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"stage2_gendered_abuse.pth\"))\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"AlbertForSequenceClassification(\n  (albert): AlbertModel(\n    (embeddings): AlbertEmbeddings(\n      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (encoder): AlbertTransformer(\n      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n      (albert_layer_groups): ModuleList(\n        (0): AlbertLayerGroup(\n          (albert_layers): ModuleList(\n            (0): AlbertLayer(\n              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (attention): AlbertSdpaAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (attention_dropout): Dropout(p=0, inplace=False)\n                (output_dropout): Dropout(p=0, inplace=False)\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              )\n              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n              (activation): GELUActivation()\n              (dropout): Dropout(p=0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (pooler): Linear(in_features=768, out_features=768, bias=True)\n    (pooler_activation): Tanh()\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report\nfrom tqdm import tqdm\n\ndef inference_on_gendered_abuse_test(model, test_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing on Gendered Abuse\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    f1_macro = f1_score(all_labels, all_preds, average='macro')\n    print(f\"\\nTest F1 Score (Macro): {f1_macro:.4f}\")\n    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, digits=4))\n    return f1_macro\n\nf1_macro_test = inference_on_gendered_abuse_test(model, gendered_test_loader)\nprint(f\"Final Gendered Abuse Test F1 Score (Macro): {f1_macro_test:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-25T12:49:00.440134Z","iopub.execute_input":"2025-03-25T12:49:00.440409Z","iopub.status.idle":"2025-03-25T12:49:04.882445Z","shell.execute_reply.started":"2025-03-25T12:49:00.440383Z","shell.execute_reply":"2025-03-25T12:49:04.881535Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                          ","output_type":"stream"},{"name":"stdout","text":"\nTest F1 Score (Macro): 0.7719\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0     0.7661    0.8188    0.7916       596\n           1     0.7831    0.7236    0.7522       539\n\n    accuracy                         0.7736      1135\n   macro avg     0.7746    0.7712    0.7719      1135\nweighted avg     0.7742    0.7736    0.7729      1135\n\nFinal Gendered Abuse Test F1 Score (Macro): 0.7719\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":22}]}